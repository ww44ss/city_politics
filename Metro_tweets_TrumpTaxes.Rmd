---
title: "Metro Areas with #TrumpTaxes tweet flux"
author: "Winston Saunders"
date: "Oct 3, 2016"
output: 
    html_document:
        css: markdown7.css
        toc: true
        toc_depth: 1
        keep_md: true
---

# Rev
    0.1 28 Sept 2016 supporess printing of all the code
    0.2 30 Sept 2016 use metro area populations from wikipedia
    0.5 1 Oct 2016 add ChoroplethRmaps to reduce 'chart junk' and add date span
    

# Set-up




```{r setup, include=TRUE, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.align='center', comment = "")
```


```{r libraries}
## sorry this is a mess. I hacked another project

library(ggmap)
library(dplyr)
library(googleway)
library(tidyr)
library(stringr)
library(xtable)
    options(xtable.floating = FALSE)
    options(xtable.timestamp = "")

library(googleway)
library(tidyr)
library(stringr)
library(gridExtra)
library(twitteR)
require(plyr)
library(methods)
library(maps)

    library(choroplethrMaps)
    
library(RSQLite)
    
    options(scipen=999)
```

set up twitter 

```{r "set up twitter API", echo=FALSE, message=FALSE, warning=FALSE}
library(twitteR)
## create URLs
reqURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"

## read file containing secret keys (obtained from apps.twitter.com)
keys <- read.table("/users/winstonsaunders/documents/city_politics/secret_t.key.txt", stringsAsFactors = FALSE, col.names = "secret" )
## convert to characters (read.table coerces to a factor)
consumerKey       <- keys$secret[1] 
consumerSecret    <- keys$secret[2] 
accessToken       <- keys$secret[3] 
accessTokenSecret <- keys$secret[4] 

## set up authentication
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessTokenSecret)
```

Test with latest George Takei tweet

```{r echo = FALSE}
## test functionanlity with georgetakei tweets
userTimeline(getUser('georgetakei'), n=1, includeRts=FALSE, excludeReplies=FALSE)[[1]]
```

Get city geo data from `maps::cities` and combine with metro-area population data

```{r}
data(state.map)
```

```{r echo=FALSE, results='asis'}
    require(maps)

    ## use maps data for geo-location
    cities <- us.cities
    
    ## create city_name by removing state designnator 
    
    cities <- cities %>% 
        mutate(city_name = gsub(' [A-Z]{2,}','', name)) %>%
        ## abbreviate Saint 
        mutate(city_name = gsub('Saint', 'St.', city_name)) %>%
        ## correct spelling error in package
        mutate(city_name = gsub('MacAllen', 'McAllen', city_name)) 
    
    ## clean
    cities <- cities[complete.cases(cities),] %>% as_data_frame
    ## sort
    cities <- cities[order(cities$pop, decreasing = TRUE), ]
    ## create a metro_match column for joining and get rid of capital
    cities <- cities %>% 
        mutate(metro_match = city_name %>% tolower) %>% 
        select(-capital) 

    
```


```{r}
    ## Use data scrapped from wikipedia for metro polulations

    metro_data <- read.csv("/users/winstonsaunders/documents/city_politics/metro_pop_table_2015.csv")
    metro_data <- metro_data %>% 
        as_data_frame %>% 
        select(-X) %>%
        ## create metro_match column
        mutate(metro_match = tolower(metro))
    

```

```{r}
    ## merge datasets    

    library(stats)
    metro_cities <- left_join(metro_data, cities, by = "metro_match")
    
    metro_cities <- metro_cities %>% select(-metro_match)

    metros <- metro_cities[complete.cases(metro_cities),]
    metros <- metros %>% 
        mutate(metro = as.character(metro))
    
    bad_metros <- metro_cities[!complete.cases(metro_cities),]
```

Select number of cities

```{r echo = 2, "get tweets", message=FALSE, warning=FALSE}
## pick number of metro areas to interrogate
    n.cities <- 40
    if (n.cities > nrow(cities)) n.cities <- nrow(cities)
```

The top Cities are:

```{r}
    metro_area_tibble <- metros %>% filter(rank <= n.cities)
    ## as a quality check nothing less than #120.
    ## there is still work to be done on 

    ## to make legacy code work assign to variable name `cities`
    cities <- metro_area_tibble

```

```{r, results='asis'}
print(xtable(head(cities[,c(1,2,3,7,8)], 10)), type="html", include.rownames=FALSE, include.colnames = TRUE)
```

Data collection for the top `r n.cities` cities (by population) in the U.S. This includes cities from `r cities$name[1]` to `r cities$name[n.cities]`.

Keeping first `r n.cities` metro areas comprises a total population of `r (sum(cities$population)/10^6) %>% round(1)` million people. 




## search

```{r "set up tweet search", echo=TRUE}

## set up
cc<-NULL
n.days <- 4
avg.period <- 2


for (day in n.days:1){

        ## set up search terms
        searchString.x <- "#Alicia"    # search term
        n.x <- 3000                     # number of tweets
        radius <- "30mi"               # radius around selected geo-location
        days.ago <-day
        duration.days <- 1            # how many days
        since.date <- (Sys.Date() - avg.period*days.ago) %>% as.character # calculated starting date
        until.date <- (Sys.Date() - avg.period*days.ago + avg.period*duration.days) %>% as.character # calculated ending date
        
        ## create dataframe for storing data
        collected_df <- data_frame("n" = 1:n.cities, 
                                   "lat" = rep_len(1., n.cities),
                                   "lon" = rep_len(1., n.cities),
                                   "tweets" = as.list(rep_len(c("a","b"), n.cities)),
                                   "n.tweets" = length(tweets),
                                   "population" = rep_len(1., n.cities),
                                   "name" = rep_len("a", n.cities),
                                   "date" = rep_len(since.date, n.cities))
        
        ## loop thru cities to get data
        ## this takes some time
        for (i in 1:n.cities){
            latitude <- cities$lat[i]
            longitude <- cities$long[i]
            
            geocode.x <- paste0(latitude, "," , longitude, "," , radius)
            
            # Twitter API limit: 180 calls / 15 minutes = 12 seconds for steady state
            Sys.sleep(2.)    
            ## get the tweets
            tweets <- searchTwitter(searchString = searchString.x , n = n.x , geocode = geocode.x, since = since.date, until = until.date, retryOnRateLimit=120)
            
            ## assign to data frame   
                collected_df[i,1] <- i
                collected_df[i,2] <- latitude
                collected_df[i,3] <- longitude
                #collected_df[i,4] <- tweets
                collected_df[i,5] <- length(tweets)
                collected_df[i,6] <- cities$population[i]
                collected_df[i,7] <- cities$metro[i] 
            }
        
        cc <- bind_rows(cc, collected_df)
}

collected_df <- cc %>% select(-tweets)

write_csv(collected_df, "/users/winstonsaunders/documents/city_politics/trumptax2.csv")



```



```{r "analyze data", echo=FALSE}

analyzed_df <- collected_df %>% 
    mutate("rel.interest" = 10^6 * n.tweets/population/duration.days ) %>% 
    select(name, lon, lat, rel.interest, n.tweets, population, date)
```

# Tweet-Map for `r searchString.x`?

```{r echo = FALSE, fig.align='center'}
    library(ggmap)
    
    #map = get_googlemap(center =  c(lon = -95.58, lat = 36.83),
    #        zoom = 3, size = c(390, 250), scale = 2, source = "google",
    #        maptype="roadmap", color = "bw" ) #, key = my.secret.key)
    #map.plot <- ggmap(map)

```


```{r echo=TRUE, fig.align='center', fig.align='center', fig.height=4, fig.width=7}
    #map.plot +  ## use this to underlay with google map
    ggplot() +   ## use this to underlay with simple border outlines
    geom_polygon(data = state.map %>% filter(region != "alaska" & region != "hawaii"), aes(x=long, y=lat, group = group), fill = "#ACC5A188", color = "#FFFFFF", size =0.2) +
    geom_point(aes(x = lon, y = lat, fill = rel.interest, size = n.tweets), data=analyzed_df, pch=21, color = "#555555") +
    ggtitle(paste0(searchString.x, " interest surges")) +
    scale_fill_gradient(low = "#72A0DD", high = "#D32F2A", space = "Lab", na.value = "grey50", guide = "colourbar") +
    theme_minimal() +
    xlab("") + ylab("") +
    theme(panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank(),
          axis.title.x=element_blank(),
          axis.text.x=element_blank(),
          axis.ticks.x=element_blank(),
          axis.title.y=element_blank(),
          axis.text.y=element_blank(),
          axis.ticks.y=element_blank()) +
    coord_fixed(ratio = 1.3, xlim = NULL, ylim = NULL) +
    facet_wrap(~date, nrow=2)

```



## `r searchString.x` AMB tweet-flux

Here are the top few cities by tweet flux (in "twipermipeds").

```{r, results='asis'}
analyzed_df$population <- analyzed_df$population %>% as.integer

print(xtable(head(analyzed_df[order(analyzed_df$rel.interest, decreasing = TRUE),c(-2,-3)], min(15, nrow(analyzed_df)))), type="html", include.rownames=FALSE, include.colnames = TRUE)
```

## `r searchString.x` AMB tweet count

Here are the top few cities sorted by raw tweets, again with major metro areas leading. Note that some other cities, like Chicago, have a large number of tweets but a lower flux because of their higher population.

```{r, results='asis'}
print(xtable(head(analyzed_df[order(analyzed_df$n.tweets, decreasing = TRUE),c(-2,-3)], min(15, nrow(analyzed_df)))), type="html", include.rownames=FALSE, include.colnames = TRUE)
```






---
title: "High Performance Reverse-Geo-Coding Model"
author: "Winston Saunders"
date: "September 19, 2016"
output: 
    html_document:
        css: markdown7.css
        toc: true
        toc_depth: 3
        keep_md: true
---

###Summary

  
###Problem Statement


```{r setup, include=TRUE, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.align='center')
```



```{r libraries}
library(ggmap)
library(dplyr)
library(googleway)
#library(choroplethrMaps)
#library(nnet)
#library(SDMTools)
library(tidyr)
library(stringr)
library(xtable)
    options(xtable.floating = FALSE)
    options(xtable.timestamp = "")
#library(gridExtra)
    
#    options(scipen=999)
    
    
#library(ggmap)
#library(dplyr)
#library(randomForest)
#library(ggmap)
library(googleway)
#library(choroplethrMaps)
#library(nnet)
#library(SDMTools)
library(tidyr)
library(stringr)
#library(xtable)
#    options(xtable.floating = FALSE)
#    options(xtable.timestamp = "")
library(gridExtra)
library(twitteR)
require(plyr)
library(methods)
library(maps)
    
    options(scipen=999)
```



## Get city data

City coordinates and populations are available in the [`{maps}`](https://cran.r-project.org/web/packages/maps/index.html) package. The package is super-convenient as it contains all the information we need for plotting and normalizing the tweet data. 

```{r echo=TRUE, results='asis'}

    require(maps)
    cities <- us.cities
    
    ## create city_name by removing state designnator
    cities <- cities %>% mutate(city_name = gsub(' [A-Z]{2,}','', name))
    ## clean
    cities <- cities[complete.cases(cities),] %>% as_data_frame
    ## sort
    cities <- cities[order(cities$pop, decreasing = TRUE), ]

```
 
The top cities by population are:  

```{r, results='asis'}
print(xtable(head(cities, 10), align = c("r","r", "c", "c", "c", "c", "c", "c")), type="html", include.rownames=FALSE, include.colnames = TRUE)
```

## Set up twitted API using the twitteR package

Setting up the twitter API is relatively quick. We can test package functionality by checking Michelle Obama's latest tweet.

```{r "set up twitter API", echo=TRUE}
library(twitteR)
## create URLs
reqURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"

## read file containing secret keys (obtained from apps.twitter.com)
keys <- read.table("/users/winstonsaunders/documents/city_politics/secret_t.key.txt")
## convert to characters
consumerKey <- keys[1,]%>%as.character()
consumerSecret <- keys[2,]%>%as.character()
accessToken <- keys[3,]%>%as.character()
accessTokenSecret <- keys[4,]%>%as.character()

## set up authentication
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessTokenSecret)

## test functionanlity
userTimeline(getUser('michelleobama'), n=1, includeRts=FALSE,excludeReplies=FALSE)

```

## Getting the data

To get the tweet data use `twitteR::searchTwiter` command. 
Data collection with the following variables. Note the radius, which is used to localize tweet collected around specific geo-locations. In this initial case I chose a radius of 30 miles. For cases where major cities are in close proximity, this may pick up some redundant tweets.  

```{r "set up tweet search", echo=TRUE}

## set up search terms
searchString.x <- "#rstats"
n.x <- 300
radius <- "15mi"
duration.days <- 7
since.date <- (Sys.Date() - duration.days) %>% as.character

```

Collected data are put into a dataframe `collected_df`. For this first pass analysis tweets are counted but not cached.   

```{r "get tweets"}
n.cities <- 80

if (n.cities > nrow(cities)) n.cities <- nrow(cities)

collected_df <- data_frame("n" = 1:n.cities, 
                           "lat" = rep_len(1., n.cities),
                           "lon" = rep_len(1., n.cities),
                           "tweets" = as.list(rep_len(c("a","b"), n.cities)),
                           "n.tweets" = length(tweets),
                           "population" = rep_len(1., n.cities),
                           "name" = rep_len("a", n.cities))

for (i in 1:n.cities){
    latitude <- cities$lat[i]
    longitude <- cities$long[i]
    
    geocode.x <- paste0(latitude, "," , longitude, "," , radius)
        
        Sys.sleep(2.)    # Twitter API limit: 180 calls / 15 minutes = 12 seconds
        
        tweets <- searchTwitter(searchString = searchString.x , n = n.x , geocode = geocode.x, since = since.date)
        
        collected_df[i,1] <- i
        collected_df[i,2] <- latitude
        collected_df[i,3] <- longitude
        #collected_df[i,4] <- tweets
        collected_df[i,5] <- length(tweets)
        collected_df[i,6] <- cities$pop[i]
        collected_df[i,7] <- cities$name[i]
    }



```

Once collected, the data are lightly analyzed. Specifically the 'tweet.density', representing the number of tweets per million people per day, is computed.

```{r "analyze data", echo=TRUE}

analyzed_df <- collected_df %>% 
    mutate("tweet.density" = 10^6 * n.tweets/population/duration.days ) %>% 
    select(name, lon, lat, tweet.density, n.tweets, population)


```

## Tweet Map

Mapping uses the `{ggmap}` package. 

```{r fig.align='center'}

#, size = c(450, 340)

library(ggmap)
    map = get_googlemap(center =  c(lon = -95.58, lat = 36.83), 
              zoom = 3, size = c(390, 250), scale = 2, source = "google",
              maptype="roadmap") #, key = my.secret.key)

    map.plot <- ggmap(map)


    map.plot + 
    geom_point(aes(x = lon, y = lat, fill = tweet.density, size = n.tweets), data=analyzed_df, pch=21, color = "#33333399") +
        ggtitle(paste0(searchString.x, " tweets for ", duration.days," days since ", since.date)) + 
        scale_fill_gradient(low = "#BBBBFF", high = "#EE3300", space = "Lab", na.value = "grey50", guide = "colourbar")

```

here are the top few cities by tweet density

```{r, results='asis'}
print(xtable(head(analyzed_df[order(analyzed_df$tweet.density, decreasing = TRUE),c(-2,-3)], min(15, nrow(analyzed_df)))), type="html", include.rownames=FALSE, include.colnames = TRUE)
```

here are the top few cities by tweets

```{r, results='asis'}
print(xtable(head(analyzed_df[order(analyzed_df$n.tweets, decreasing = TRUE),c(-2,-3)], min(15, nrow(analyzed_df)))), type="html", include.rownames=FALSE, include.colnames = TRUE)
```






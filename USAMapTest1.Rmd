---
title: "High Performance Reverse-Geo-Coding Model"
author: "Winston Saunders"
date: "September 22, 2016"
output: 
    html_document:
        css: markdown7.css
        toc: true
        toc_depth: 3
        keep_md: true
---

# Summary

A quesiton was asked on the Sept ["Not So Standard Deviations"](https://www.patreon.com/NSSDeviations) podcast about the "best city for data science." To inject some analysis in the discussion, I used the `twitteR` package to measure interest in __R__ by computing the frequency and density of tweets with the `#rstats` hashtag. Surprisingly, Boston, with a tweet density of over 75 #rstat tweets per million residents per day ("twepermipeds"), leads the bunch. Other leading cities include Jersey City and Newark, Long Beach and Oakland, Washington DC, Seattle, Raleigh NC, San Francisco, and Henderson NV.  Portland, Oregon weighs-in within the top 15.
Results are sensitive to assumptions about metro size.  
This was quick and dirty so no telling how stable the result will be over time.  

# Problem Statement

On their September Podcast, Hilary and Roger discussed the best city for data science. I've never met a person from SF who didn't think SF was the best at everything, so I decided to try measuring something just to double check.  

```{r setup, include=TRUE, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.align='center')
```


```{r libraries}
## sorry this is a mess. I hacked another project

library(ggmap)
library(dplyr)
library(googleway)
library(tidyr)
library(stringr)
library(xtable)
    options(xtable.floating = FALSE)
    options(xtable.timestamp = "")

library(googleway)

library(tidyr)
library(stringr)

library(gridExtra)
library(twitteR)
require(plyr)
library(methods)
library(maps)
    
    options(scipen=999)
```



# How do I get City Data?

City coordinates and populations are available in the super-convenient  [`{maps}`](https://cran.r-project.org/web/packages/maps/index.html) package, which contains all the information we need for plotting and normalizing tweet data. 

```{r echo=TRUE, results='asis'}

    require(maps)
    cities <- us.cities
    
    ## create city_name by removing state designnator
    cities <- cities %>% mutate(city_name = gsub(' [A-Z]{2,}','', name))
    ## clean
    cities <- cities[complete.cases(cities),] %>% as_data_frame
    ## sort
    cities <- cities[order(cities$pop, decreasing = TRUE), ]

```
 
The top cities by population are:  

```{r, results='asis'}
print(xtable(head(cities, 10), align = c("r","r", "c", "c", "c", "c", "c", "c")), type="html", include.rownames=FALSE, include.colnames = TRUE)
```

# What is required to set up the twitted API using the twitteR package?

Setting up the twitter API is relatively quick. Package functionality is tested by checking Michelle Obama's latest tweet.

```{r "set up twitter API", echo=TRUE}
library(twitteR)
## create URLs
reqURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"

## read file containing secret keys (obtained from apps.twitter.com)
keys <- read.table("/users/winstonsaunders/documents/city_politics/secret_t.key.txt")
## convert to characters
consumerKey <- keys[1,]%>%as.character()
consumerSecret <- keys[2,]%>%as.character()
accessToken <- keys[3,]%>%as.character()
accessTokenSecret <- keys[4,]%>%as.character()

## set up authentication
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessTokenSecret)

## test functionanlity
userTimeline(getUser('michelleobama'), n=1, includeRts=FALSE,excludeReplies=FALSE)

```

# Getting the data

To get the tweet data use the `twitteR::searchTwiter` command. 
Data collection is with the following variables. Note the radius of 20 miles, which is used to localize tweet collected around specific geo-locations. In this amazingly crude initial case I chose a radius of 20 miles. For cases where major cities are in close proximity, this certainly picks up some redundant tweets. Life in the big leagues.

```{r "set up tweet search", echo=TRUE}

## set up search terms
searchString.x <- "#rstats"    # search term
n.x <- 500                     # number of tweets
radius <- "20mi"               # radius around selected geo-location
duration.days <- 7             # how many days
since.date <- (Sys.Date() - duration.days) %>% as.character # calculated starting date

```



```{r "get tweets"}
n.cities <- 80

if (n.cities > nrow(cities)) n.cities <- nrow(cities)

collected_df <- data_frame("n" = 1:n.cities, 
                           "lat" = rep_len(1., n.cities),
                           "lon" = rep_len(1., n.cities),
                           "tweets" = as.list(rep_len(c("a","b"), n.cities)),
                           "n.tweets" = length(tweets),
                           "population" = rep_len(1., n.cities),
                           "name" = rep_len("a", n.cities))

for (i in 1:n.cities){
    latitude <- cities$lat[i]
    longitude <- cities$long[i]
    
    geocode.x <- paste0(latitude, "," , longitude, "," , radius)
        
        Sys.sleep(2.)    # Twitter API limit: 180 calls / 15 minutes = 12 seconds
        
        tweets <- searchTwitter(searchString = searchString.x , n = n.x , geocode = geocode.x, since = since.date)
        
        collected_df[i,1] <- i
        collected_df[i,2] <- latitude
        collected_df[i,3] <- longitude
        #collected_df[i,4] <- tweets
        collected_df[i,5] <- length(tweets)
        collected_df[i,6] <- cities$pop[i]
        collected_df[i,7] <- cities$name[i]
    }



```

I pull data for the top `r n.cities` cities (by population) in the U.S. This includes cities from `r collected_df$name[1]` to `r collected_df$name[n.cities]`.

# Analysis

Once collected, the data are lightly analyzed. Specifically the 'tweet.density', representing the number of tweets per million people per day ("twepermipeds"), is computed.

```{r "analyze data", echo=TRUE}

analyzed_df <- collected_df %>% 
    mutate("tweet.density" = 10^6 * n.tweets/population/duration.days ) %>% 
    select(name, lon, lat, tweet.density, n.tweets, population)


```



Collected data are put into  `collected_df`. For this first-pass analysis tweets are counted but are not cached. 

# What is the Tweet Map?

Mapping uses the `{ggmap}` package. 

```{r fig.align='center'}

#, size = c(450, 340)

library(ggmap)
    map = get_googlemap(center =  c(lon = -95.58, lat = 36.83), 
              zoom = 3, size = c(390, 250), scale = 2, source = "google",
              maptype="roadmap") #, key = my.secret.key)

    map.plot <- ggmap(map)


    map.plot + 
    geom_point(aes(x = lon, y = lat, fill = tweet.density, size = n.tweets), data=analyzed_df, pch=21, color = "#33333399") +
        ggtitle(paste0(searchString.x, " tweets for ", duration.days," days since ", since.date)) + 
        scale_fill_gradient(low = "#BBBBFF", high = "#EE3300", space = "Lab", na.value = "grey50", guide = "colourbar")

```

# What are the top cities by tweets and densities?

here are the top few cities by tweet density (tweet per million per day, or "twepermipeds")

```{r, results='asis'}
print(xtable(head(analyzed_df[order(analyzed_df$tweet.density, decreasing = TRUE),c(-2,-3)], min(15, nrow(analyzed_df)))), type="html", include.rownames=FALSE, include.colnames = TRUE)
```

here are the top few cities sorted by raw tweets

```{r, results='asis'}
print(xtable(head(analyzed_df[order(analyzed_df$n.tweets, decreasing = TRUE),c(-2,-3)], min(15, nrow(analyzed_df)))), type="html", include.rownames=FALSE, include.colnames = TRUE)
```






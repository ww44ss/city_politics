---
title: "Leading Cities in #MAGA tweet flux"
author: "Winston Saunders"
date: "September 23, 2016"
output: 
    html_document:
        css: markdown7.css
        toc: true
        toc_depth: 1
        keep_md: true
---

# setup


```{r setup, include=TRUE, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.align='center', comment = "")
```


```{r libraries}
## sorry this is a mess. I hacked another project

library(ggmap)
library(dplyr)
library(googleway)
library(tidyr)
library(stringr)
library(xtable)
    options(xtable.floating = FALSE)
    options(xtable.timestamp = "")

library(googleway)

library(tidyr)
library(stringr)

library(gridExtra)
library(twitteR)
require(plyr)
library(methods)
library(maps)
    
    options(scipen=999)
```

```{r "set up tweet search", echo=TRUE}

## set up search terms
searchString.x <- "#GenniferFlowers"    # search term
n.x <- 3000                     # number of tweets
radius <- "10mi"               # radius around selected geo-location
duration.days <- 1             # how many days
since.date <- (Sys.Date() - duration.days) %>% as.character # calculated starting date

```

# `r searchString.x` geo-preference.

```{r "set up twitter API", echo=FALSE, message=FALSE, warning=FALSE}
library(twitteR)
## create URLs
reqURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"

## read file containing secret keys (obtained from apps.twitter.com)
keys <- read.table("/users/winstonsaunders/documents/city_politics/secret_t.key.txt", stringsAsFactors = FALSE, col.names = "secret" )
## convert to characters (read.table coerces to a factor)
consumerKey       <- keys$secret[1] 
consumerSecret    <- keys$secret[2] 
accessToken       <- keys$secret[3] 
accessTokenSecret <- keys$secret[4] 

## set up authentication
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessTokenSecret)
```



```{r echo = FALSE}
## test functionanlity with georgetakei tweets
#userTimeline(getUser('georgetakei'), n=1, includeRts=FALSE, excludeReplies=FALSE)[[1]]
```

 

```{r echo=FALSE, results='asis'}

    require(maps)
    cities <- us.cities
    
    ## create city_name by removing state designnator
    cities <- cities %>% mutate(city_name = gsub(' [A-Z]{2,}','', name))
    ## clean
    cities <- cities[complete.cases(cities),] %>% as_data_frame
    ## sort
    cities <- cities[order(cities$pop, decreasing = TRUE), ]
    
    
    MetroVector <- function(cities, city_name = "Los Angeles", state = "CA", longmin = -118.50, longmax = -117.1, latmax = 34.27, latmin = 33.27) {
        
        ##  function MetroVec
        ##      computes metro area data
        ##  inputs: 
        ##      cities: tibble
        ##      city_name a character vector of the output city name
        ##      state a two letter designator
        ##      longmin min lognitude
        ##      longmx max longitude
        ##      latmax max latitude
        ##      latmin min latitude
        ##  output:
        ##      an updated tibble with the population sum within geo boundary
        ##      and a geolocationweighted by population

        
        Metro <- filter(cities, long > longmin & long < longmax & lat < latmax & lat > latmin)
        
        ## compute sums and weighted locations
        MetroPop <- sum(Metro$pop)
        MetroLong <- sum(Metro$long * Metro$pop)/MetroPop %>% round(2)
        LAMetroLat <- sum(Metro$lat * Metro$pop)/MetroPop %>% round(2)
        
        Metro_tibl <- tibble(name = city_name, coutry.etc = state, pop = LAMetroPop, lat = LAMetroLat, long = LAMetroLong, capital = 0 )
        
        cities.temp <- filter(cities, !(long > longmin & long < longmax & lat < latmax & lat > latmin))
    
        cities.temp <- rbind.tbl_df(cities.temp, MetroVec)
        
        return(cities.temp)
        
    }
    
    cities <- MetroVector(cities, "Los Angeles", "CA", longmin = -118.50, longmax = -117.1, latmax = 34.27, latmin = 33.27)
    
    cities <- MetroVector(cities, "San Diego", "CA", longmin = -117.29, longmax = -116.88, latmax = 32.92, latmin = 32.54)
    
    
    
        
        NYMetro <- filter(cities, long > -118.50 & long < -117.1 & lat < 34.27 & lat > 33.27)
    
        NYMetroPop <- sum(NYMetro$pop)
        MYMetroLong <- sum(LAMetro$long * LAMetro$pop)/LAMetroPop %>% round(2)
        LAMetroLat <- sum(LAMetro$lat * LAMetro$pop)/LAMetroPop %>% round(2)
        
        LAMetroVec <- c("Los Angeles Metro CA", "CA", LAMetroPop, LAMetroLat, LAMetroLong, 0 )
        
        cities <- filter(cities, !(long > -118.50 & long < -117.1 & lat < 34.27 & lat > 33.27))
    
        cities <- rbind(LAMetroVec, cities)   
        
    
    ## compute for metro areas
    LAMetroPop <- cities$pop[cities$city_name == "Los Angeles" ] +
        cities$pop[cities$city_name == "Irvine" ] +
        cities$pop[cities$city_name == "Glendale" ] +
        cities$pop[cities$city_name == "Riverside" ] + 
        cities$pop[cities$city_name == "Santa Ana" ] + 
        cities$pop[cities$city_name == "Anaheim" ] +
        cities$pop[cities$city_name == "Glendale" ] +
        cities$pop[cities$city_name == "Newport Beach" ] +
        cities$pop[cities$city_name == "Lake Forest" ] +
        cities$pop[cities$city_name == "Rancho Santa Margarita" ] +
        cities$pop[cities$city_name == "Costa Mesa" ] +
        cities$pop[cities$city_name == "Lake Elsinore" ] +
        cities$pop[cities$city_name == "La Quinta" ] +
        cities$pop[cities$city_name == "Huntington Beach" ] +
        cities$pop[cities$city_name == "Glendale" ] +
        cities$pop[cities$city_name == "Glendale" ]
    
    SanDiegoMetro <-  
        cities$pop[cities$city_name == "Chula Vista" ] + 
        cities$pop[cities$city_name == "San Diego" ] + 
        cities$pop[cities$city_name == "National City" ] +
        cities$pop[cities$city_name == "La Mesa" ] +
        cities$pop[cities$city_name == "El Cajon" ] +
        cities$pop[cities$city_name == "Santee" ] +
        cities$pop[cities$city_name == "Poway" ] +
        cities$pop[cities$city_name == "Encinitas" ] +
        cities$pop[cities$city_name == "Carlsbad" ]
        
        
    

```
 


```{r, results='asis'}
#print(xtable(head(cities, 4), align = c("r","r", "c", "c", "c", "c", "c", "c")), type="html", include.rownames=FALSE, include.colnames = TRUE)
```

Use the `twitteR::searchTwitter` command. 

```{r echo = 1, "get tweets", message=FALSE, warning=FALSE}
n.cities <- 100

if (n.cities > nrow(cities)) n.cities <- nrow(cities)

collected_df <- data_frame("n" = 1:n.cities, 
                           "lat" = rep_len(1., n.cities),
                           "lon" = rep_len(1., n.cities),
                           "tweets" = as.list(rep_len(c("a","b"), n.cities)),
                           "n.tweets" = length(tweets),
                           "population" = rep_len(1., n.cities),
                           "name" = rep_len("a", n.cities))

for (i in 1:n.cities){
    latitude <- cities$lat[i]
    longitude <- cities$long[i]
    
    ## increase search radius for these cities
    temp.radius <- radius
    if (cities$city_name == "New York") temp.radius <- 20
    if (cities$city_name == "Los Angeles") temp.radius <- 30
    if (cities$city_name == "San Francisco") temp.radius <- 20
    if (cities$city_name == "WASHINGTON") temp.radius <- 20
    
    
    
    geocode.x <- paste0(latitude, "," , longitude, "," , temp.radius)
        # Twitter API limit: 180 calls / 15 minutes = 12 seconds for steady state
        Sys.sleep(4.)    
        
        tweets <- searchTwitter(searchString = searchString.x , n = n.x , geocode = geocode.x, since = since.date, retryOnRateLimit=120)
        
        collected_df[i,1] <- i
        collected_df[i,2] <- latitude
        collected_df[i,3] <- longitude
        #collected_df[i,4] <- tweets
        collected_df[i,5] <- length(tweets)
        collected_df[i,6] <- cities$pop[i]
        collected_df[i,7] <- cities$name[i]
    }



```

Data collection for the top `r n.cities` cities (by population) in the U.S. This includes cities from `r collected_df$name[1]` to `r collected_df$name[n.cities]`.

```{r "analyze data", echo=FALSE}

analyzed_df <- collected_df %>% 
    mutate("tweet.flux" = 10^6 * n.tweets/population/duration.days ) %>% 
    select(name, lon, lat, tweet.flux, n.tweets, population)
```

# Tweet-Map for `r searchString.x`?

```{r echo = FALSE, fig.align='center'}
    library(ggmap)
    map = get_googlemap(center =  c(lon = -95.58, lat = 36.83), 
              zoom = 3, size = c(390, 250), scale = 2, source = "google",
              maptype="roadmap") #, key = my.secret.key)
    map.plot <- ggmap(map)
```


```{r echo=TRUE, fig.align='center'}
map.plot +
    geom_point(aes(x = lon, y = lat, fill = tweet.flux, size = n.tweets), data=analyzed_df, pch=21, color = "#33333399") +
    ggtitle(paste0(searchString.x, " tweets in ", duration.days," days since ", since.date, " r = ", radius)) +
    scale_fill_gradient(low = "#BBBBFF", high = "#EE3300", space = "Lab", na.value = "grey50", guide = "colourbar")

```



## `r searchString.x` AMB tweet-flux

Here are the top few cities by tweet flux (in "twipermipeds").

```{r, results='asis'}
analyzed_df$population <- analyzed_df$population %>% as.integer

print(xtable(head(analyzed_df[order(analyzed_df$tweet.flux, decreasing = TRUE),c(-2,-3)], min(15, nrow(analyzed_df)))), type="html", include.rownames=FALSE, include.colnames = TRUE)
```

## `r searchString.x` AMB tweet count

Here are the top few cities sorted by raw tweets, again with major metro areas leading. Note that some other cities, like Chicago, have a large number of tweets but a lower flux because of their higher population.

```{r, results='asis'}
print(xtable(head(analyzed_df[order(analyzed_df$n.tweets, decreasing = TRUE),c(-2,-3)], min(15, nrow(analyzed_df)))), type="html", include.rownames=FALSE, include.colnames = TRUE)
```

  
  




